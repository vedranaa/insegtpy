{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df033fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using InSegt for dictionary-based segmentation of nerves images\n",
    "\n",
    "InSegt-py is a py version of [InSegt](https://github.com/vedranaa/InSegt). Basic variant of InSegt is described in our paper [Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images](http://openaccess.thecvf.com/content_CVPRW_2020/papers/w57/Dahl_Content-Based_Propagation_of_User_Markings_for_Interactive_Segmentation_of_Patterned_CVPRW_2020_paper.pdf), CVPRW 2020. But InSegt has evolved, so please check the demos and notebooks for the updated version.\n",
    "\n",
    "This is a example of interactive image segmentation with InSegt. Here we usa a model which builds dictionay by clustering image features in a k-means tree. For features we use Gaussian derivatives, where we use a few different values for the standard deviation of the Gaussian kernel. Furthermore, the model is made multi-scale by building and including sub-models operating on a downscaled version of the input image. \n",
    "\n",
    "In this example we use slices from volumetric images of peripheral nerves. The study of the data is described in the article [Three-dimensional architecture of human diabetic peripheral nerves revealed by X-ray phase contrast holographic nanotomography](https://www.nature.com/articles/s41598-020-64430-5?utm_source=other&utm_medium=other&utm_content=null&utm_campaign=JRCN_2_LW01_CN_SCIREP_article_paid_XMOL), Scientific Reports 2020.\n",
    "\n",
    "## Import packages\n",
    "Most importantly, you need `insegt` and `insegtpy.models`. You also need to be able to read in the image (for example using `PIL`), and show the result (for example using `matplotlib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import insegtpy\n",
    "import insegtpy.models\n",
    "import PIL\n",
    "import numpy as np\n",
    "import urllib.request  # for getting the image from the data repository\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e59a3",
   "metadata": {},
   "source": [
    "## Load the image \n",
    "\n",
    "Get the image from our data repository. Alternatively, load the image from your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07dbc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.qim.dk/InSegt_data/2D/NT2_0001.png'\n",
    "image = np.array(PIL.Image.open(urllib.request.urlopen(url)))\n",
    "\n",
    "# # Check the path and load the image locally\n",
    "# image = np.array(PIL.Image.open('../data/NT2_0001.png'))\n",
    "\n",
    "print(f'Image size is {image.shape}')\n",
    "print(f'Image type is {image.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758973d",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "For this model we use a km-tree with branching factor 25 and 3 layers. Gaussian features are computed using Gaussian kernels of four different standard deviatioons. 40000 feature vectors have been extracted to train the model. When propagating user labelings we use a 9x9 kernel. Finaly, for multiscale feature the model incudes a submodel build on a image resized with factor 0.5 and 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = insegtpy.models.gauss_features_segmentor(image, \n",
    "                                   branching_factor = 12, \n",
    "                                   number_layers = 4,\n",
    "                                   number_training_vectors = 50000,\n",
    "                                   features_sigma = [1,2,4,8], \n",
    "                                   propagation_size = 15, \n",
    "                                   scales = [1, 0.7, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3c769",
   "metadata": {},
   "source": [
    "# Use the model\n",
    "\n",
    "This notebook demonstrates three ways of uing segmentatiton model:\n",
    " - (A) Using the segmantation model with an interactive annotator. \n",
    " - (B) Using interactive annotator, but loading an annotation saved in a previous session, so you don't need to start from scratch every time. \n",
    " - (C) Using segmentation model in a non-interactive manner. \n",
    "\n",
    "The code below shows all three ways of using the model. You can choose to run only one of the three blocks of code. The code for the two versions of the interactive use is commented out, such that running all cells does not require interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac610bc6",
   "metadata": {},
   "source": [
    "### (A) Interactive use\n",
    "This will launch an InSegt window, where you may anotate the features of interest. We suggest that you fully anotate a small part of the image surounding one or two nerve cells. For on-screen help when using InSegt, hold ´H´. When you are sattisfied with the resulting segmentation, close InSegt window. You will be able to access the segmentation result via InSegt object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex = insegtpy.insegt(image, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab789b",
   "metadata": {},
   "source": [
    "You can get the segmentation from the probabilities attribute of the interactive annotator. (Alternatively, you can always get the probabilities and the segmentation by passing the image to the model, as shown for the non-interactive use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fd683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg = insegtpy.utils.segment_probabilities(ex.probabilities) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b3ad3",
   "metadata": {},
   "source": [
    "### (B) Interactive use with labeling\n",
    "If you have a labeling, for example from a previous interactive session, you can load in in the interactive annotator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55218f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://data.qim.dk/InSegt_data/2D/NT2_0001_labels.png'\n",
    "# labels = np.array(PIL.Image.open(urllib.request.urlopen(url)))\n",
    "# ex = insegtpy.insegt(image, model, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435687e7",
   "metadata": {},
   "source": [
    "Again, getting the hold of image segmentation from the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c741c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg = insegtpy.utils.segment_probabilities(ex.probabilities) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4da6c3",
   "metadata": {},
   "source": [
    "### (C) Non-interactive use\n",
    "The segmentation model may also be used non-interactively. For this you need a labeling, for example from an interactive session. The model is updated (trained) by passing the labels to the model. The model returns the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.qim.dk/InSegt_data/2D/NT2_0001_labels.png'\n",
    "labels = np.array(PIL.Image.open(urllib.request.urlopen(url)))\n",
    "seg = insegtpy.utils.segment_probabilities(model.process(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76eb805",
   "metadata": {},
   "source": [
    "## Test on another (similar) image\n",
    "\n",
    "You can also use the learned model to segment another similar image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.qim.dk/InSegt_data/2D/NT2_0512.png'\n",
    "image_new = np.array(PIL.Image.open(urllib.request.urlopen(url)))\n",
    "\n",
    "prob_new = model.segment_new(image_new)\n",
    "seg_new = insegtpy.utils.segment_probabilities(prob_new)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, sharex = True, sharey = True )\n",
    "ax[0][0].imshow(image)\n",
    "ax[1][0].imshow(seg)\n",
    "ax[1][0].set_title('Train')\n",
    "ax[0][1].imshow(image_new)\n",
    "ax[1][1].imshow(seg_new)\n",
    "ax[1][1].set_title('Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9861eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insegtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
